{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:19.768014Z",
     "iopub.status.busy": "2021-01-24T06:33:19.767214Z",
     "iopub.status.idle": "2021-01-24T06:33:19.773865Z",
     "shell.execute_reply": "2021-01-24T06:33:19.774487Z"
    },
    "papermill": {
     "duration": 0.038707,
     "end_time": "2021-01-24T06:33:19.774669",
     "exception": false,
     "start_time": "2021-01-24T06:33:19.735962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:19.833718Z",
     "iopub.status.busy": "2021-01-24T06:33:19.832915Z",
     "iopub.status.idle": "2021-01-24T06:33:19.858440Z",
     "shell.execute_reply": "2021-01-24T06:33:19.859491Z"
    },
    "papermill": {
     "duration": 0.060516,
     "end_time": "2021-01-24T06:33:19.859651",
     "exception": false,
     "start_time": "2021-01-24T06:33:19.799135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading train_csv file\n",
    "train_set = pd.read_csv('../input/hackerearth-deep-learning-challenge-holidayseason/dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:19.922470Z",
     "iopub.status.busy": "2021-01-24T06:33:19.921450Z",
     "iopub.status.idle": "2021-01-24T06:33:19.936211Z",
     "shell.execute_reply": "2021-01-24T06:33:19.937204Z"
    },
    "papermill": {
     "duration": 0.053526,
     "end_time": "2021-01-24T06:33:19.937366",
     "exception": false,
     "start_time": "2021-01-24T06:33:19.883840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image3476.jpg</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image5198.jpg</td>\n",
       "      <td>Candle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image4183.jpg</td>\n",
       "      <td>Snowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image1806.jpg</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image7831.jpg</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image          Class\n",
       "0  image3476.jpg  Miscellaneous\n",
       "1  image5198.jpg         Candle\n",
       "2  image4183.jpg        Snowman\n",
       "3  image1806.jpg  Miscellaneous\n",
       "4  image7831.jpg  Miscellaneous"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:20.002479Z",
     "iopub.status.busy": "2021-01-24T06:33:20.001743Z",
     "iopub.status.idle": "2021-01-24T06:33:20.005414Z",
     "shell.execute_reply": "2021-01-24T06:33:20.005851Z"
    },
    "papermill": {
     "duration": 0.041699,
     "end_time": "2021-01-24T06:33:20.005963",
     "exception": false,
     "start_time": "2021-01-24T06:33:19.964264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'Class'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 2 columns given image and it's repective class\n",
    "train_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:20.040857Z",
     "iopub.status.busy": "2021-01-24T06:33:20.040239Z",
     "iopub.status.idle": "2021-01-24T06:33:20.936260Z",
     "shell.execute_reply": "2021-01-24T06:33:20.935722Z"
    },
    "papermill": {
     "duration": 0.914834,
     "end_time": "2021-01-24T06:33:20.936392",
     "exception": false,
     "start_time": "2021-01-24T06:33:20.021558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get some intution about classes of image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:20.976229Z",
     "iopub.status.busy": "2021-01-24T06:33:20.975712Z",
     "iopub.status.idle": "2021-01-24T06:33:21.168635Z",
     "shell.execute_reply": "2021-01-24T06:33:21.169058Z"
    },
    "papermill": {
     "duration": 0.216709,
     "end_time": "2021-01-24T06:33:21.169176",
     "exception": false,
     "start_time": "2021-01-24T06:33:20.952467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#store frequency of image in given class\n",
    "label_counts = train_set['Class'].value_counts()\n",
    "\n",
    "#initalize plot figure of given size\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "#draw image of label_counts using seaborn barplot\n",
    "sns.barplot(label_counts.index,label_counts.values)\n",
    "\n",
    "#lable x-axis\n",
    "plt.xlabel('Image Class', fontsize =12)\n",
    "\n",
    "#lable y-axis\n",
    "plt.ylabel('frequency', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:21.206117Z",
     "iopub.status.busy": "2021-01-24T06:33:21.205553Z",
     "iopub.status.idle": "2021-01-24T06:33:21.381434Z",
     "shell.execute_reply": "2021-01-24T06:33:21.380912Z"
    },
    "papermill": {
     "duration": 0.195539,
     "end_time": "2021-01-24T06:33:21.381553",
     "exception": false,
     "start_time": "2021-01-24T06:33:21.186014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to read images\n",
    "import cv2\n",
    "\n",
    "#use to show progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:21.422783Z",
     "iopub.status.busy": "2021-01-24T06:33:21.420798Z",
     "iopub.status.idle": "2021-01-24T06:33:21.423432Z",
     "shell.execute_reply": "2021-01-24T06:33:21.423868Z"
    },
    "papermill": {
     "duration": 0.025353,
     "end_time": "2021-01-24T06:33:21.423979",
     "exception": false,
     "start_time": "2021-01-24T06:33:21.398626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this dataset we have different shape of image, we fix to 64*64\n",
    "img_size = 64\n",
    "\n",
    "def read_img(img_path):\n",
    "    # cv2.IMREAD_COLOR to store image in color form(length*width*numberofchannels)\n",
    "    img = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    #resize image\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:33:21.463811Z",
     "iopub.status.busy": "2021-01-24T06:33:21.463134Z",
     "iopub.status.idle": "2021-01-24T06:33:59.966832Z",
     "shell.execute_reply": "2021-01-24T06:33:59.966277Z"
    },
    "papermill": {
     "duration": 38.525951,
     "end_time": "2021-01-24T06:33:59.966973",
     "exception": false,
     "start_time": "2021-01-24T06:33:21.441022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6469/6469 [00:38<00:00, 168.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#store image in form of list of matrices\n",
    "train_img = []\n",
    "\n",
    "# store path of training image folder\n",
    "train_img_path = '../input/hackerearth-deep-learning-challenge-holidayseason/dataset/train/'\n",
    "\n",
    "for img_name in tqdm(train_set['Image'].values):\n",
    "    train_img.append(read_img(train_img_path + img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:00.235432Z",
     "iopub.status.busy": "2021-01-24T06:34:00.234346Z",
     "iopub.status.idle": "2021-01-24T06:34:00.350081Z",
     "shell.execute_reply": "2021-01-24T06:34:00.349569Z"
    },
    "papermill": {
     "duration": 0.235299,
     "end_time": "2021-01-24T06:34:00.350183",
     "exception": false,
     "start_time": "2021-01-24T06:34:00.114884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as pixel values is in between 0-255 so we divide by 255 so that range is in between 0-1\n",
    "# as it easier for your cost function to find a local or global minimum and cnn train more efficiently\n",
    "\n",
    "x_train = np.array(train_img,np.float32)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:00.587468Z",
     "iopub.status.busy": "2021-01-24T06:34:00.586635Z",
     "iopub.status.idle": "2021-01-24T06:34:00.590455Z",
     "shell.execute_reply": "2021-01-24T06:34:00.590012Z"
    },
    "papermill": {
     "duration": 0.123506,
     "end_time": "2021-01-24T06:34:00.590551",
     "exception": false,
     "start_time": "2021-01-24T06:34:00.467045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6469, 64, 64, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:00.895450Z",
     "iopub.status.busy": "2021-01-24T06:34:00.894476Z",
     "iopub.status.idle": "2021-01-24T06:34:00.911691Z",
     "shell.execute_reply": "2021-01-24T06:34:00.912497Z"
    },
    "papermill": {
     "duration": 0.206806,
     "end_time": "2021-01-24T06:34:00.912683",
     "exception": false,
     "start_time": "2021-01-24T06:34:00.705877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jacket': 1, 'Snowman': 2, 'Christmas_Tree': 3, 'Candle': 4, 'Airplane': 5, 'Miscellaneous': 6}\n",
      "Miscellaneous\n"
     ]
    }
   ],
   "source": [
    "# store classes in form of index\n",
    "label_list = train_set['Class'].tolist()\n",
    "label_numeric = {k: v+1 for v, k in enumerate(set(label_list))}\n",
    "print(label_numeric)\n",
    "\n",
    "print(label_list[0])\n",
    "\n",
    "#store it in form of numpy array\n",
    "y_train = [label_numeric[k] for k in label_list]\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:01.353450Z",
     "iopub.status.busy": "2021-01-24T06:34:01.352480Z",
     "iopub.status.idle": "2021-01-24T06:34:06.185222Z",
     "shell.execute_reply": "2021-01-24T06:34:06.184309Z"
    },
    "papermill": {
     "duration": 5.04629,
     "end_time": "2021-01-24T06:34:06.185352",
     "exception": false,
     "start_time": "2021-01-24T06:34:01.139062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:06.445735Z",
     "iopub.status.busy": "2021-01-24T06:34:06.444516Z",
     "iopub.status.idle": "2021-01-24T06:34:06.447834Z",
     "shell.execute_reply": "2021-01-24T06:34:06.447408Z"
    },
    "papermill": {
     "duration": 0.137503,
     "end_time": "2021-01-24T06:34:06.447926",
     "exception": false,
     "start_time": "2021-01-24T06:34:06.310423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert all classes of data in form of category in binary form \n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:06.695321Z",
     "iopub.status.busy": "2021-01-24T06:34:06.693530Z",
     "iopub.status.idle": "2021-01-24T06:34:06.697417Z",
     "shell.execute_reply": "2021-01-24T06:34:06.696943Z"
    },
    "papermill": {
     "duration": 0.129331,
     "end_time": "2021-01-24T06:34:06.697507",
     "exception": false,
     "start_time": "2021-01-24T06:34:06.568176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.126998,
     "end_time": "2021-01-24T06:34:06.944444",
     "exception": false,
     "start_time": "2021-01-24T06:34:06.817446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:07.195135Z",
     "iopub.status.busy": "2021-01-24T06:34:07.194511Z",
     "iopub.status.idle": "2021-01-24T06:34:13.014711Z",
     "shell.execute_reply": "2021-01-24T06:34:13.014189Z"
    },
    "papermill": {
     "duration": 5.950124,
     "end_time": "2021-01-24T06:34:13.014812",
     "exception": false,
     "start_time": "2021-01-24T06:34:07.064688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = applications.ResNet50(weights='imagenet',\n",
    "                                   include_top=False,\n",
    "                                   input_shape=(img_size,img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:13.322993Z",
     "iopub.status.busy": "2021-01-24T06:34:13.321711Z",
     "iopub.status.idle": "2021-01-24T06:34:13.324419Z",
     "shell.execute_reply": "2021-01-24T06:34:13.324839Z"
    },
    "papermill": {
     "duration": 0.128717,
     "end_time": "2021-01-24T06:34:13.324948",
     "exception": false,
     "start_time": "2021-01-24T06:34:13.196231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base_model.trainable = True\n",
    "# set_trainable = False\n",
    "# for layer in base_model.layers:\n",
    "#     if layer.name in ['res5c_branch2b', 'res5c_branch2c', 'activation_97']:\n",
    "#         set_trainable = True\n",
    "#     if set_trainable:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "# layers = [(layer, layer.name, layer.trainable) for layer in base_model.layers]\n",
    "# pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:13.577828Z",
     "iopub.status.busy": "2021-01-24T06:34:13.576803Z",
     "iopub.status.idle": "2021-01-24T06:34:13.730128Z",
     "shell.execute_reply": "2021-01-24T06:34:13.729387Z"
    },
    "papermill": {
     "duration": 0.283028,
     "end_time": "2021-01-24T06:34:13.730273",
     "exception": false,
     "start_time": "2021-01-24T06:34:13.447245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 7)            1049607     conv5_block3_out[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 24,637,319\n",
      "Trainable params: 24,584,199\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "add_model = Sequential()\n",
    "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(Dropout(0.3))\n",
    "add_model.add(Dense(128, activation='relu'))\n",
    "add_model.add(Dropout(0.3))\n",
    "add_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:14.036744Z",
     "iopub.status.busy": "2021-01-24T06:34:14.034925Z",
     "iopub.status.idle": "2021-01-24T06:34:14.152528Z",
     "shell.execute_reply": "2021-01-24T06:34:14.151977Z"
    },
    "papermill": {
     "duration": 0.244382,
     "end_time": "2021-01-24T06:34:14.152642",
     "exception": false,
     "start_time": "2021-01-24T06:34:13.908260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32      \n",
    "epochs = 200                \n",
    "\n",
    "# data augmentation\n",
    "train_datagen = ImageDataGenerator(zoom_range=0.3, rotation_range=50,\n",
    " width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, \n",
    " horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:14.401462Z",
     "iopub.status.busy": "2021-01-24T06:34:14.400900Z",
     "iopub.status.idle": "2021-01-24T06:34:14.405104Z",
     "shell.execute_reply": "2021-01-24T06:34:14.404688Z"
    },
    "papermill": {
     "duration": 0.130166,
     "end_time": "2021-01-24T06:34:14.405195",
     "exception": false,
     "start_time": "2021-01-24T06:34:14.275029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# chk_path = 'holiday.h5'\n",
    "# log_dir = 'checkpoint/logs/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath=chk_path,\n",
    "#                              save_best_only=True,\n",
    "#                              verbose=1,\n",
    "#                              mode='min',\n",
    "#                              moniter='val_loss')\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss', \n",
    "#                           min_delta=0, \n",
    "#                           patience=3, \n",
    "#                           verbose=1, \n",
    "#                           restore_best_weights=True)\n",
    "                        \n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "#                               factor=0.2, \n",
    "#                               patience=6, \n",
    "#                               verbose=1, \n",
    "#                               min_delta=0.0001)\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "# csv_logger = CSVLogger('training_log')\n",
    "\n",
    "# callbacks = [checkpoint,reduce_lr,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:34:14.658935Z",
     "iopub.status.busy": "2021-01-24T06:34:14.658424Z",
     "iopub.status.idle": "2021-01-24T07:15:38.330207Z",
     "shell.execute_reply": "2021-01-24T07:15:38.329762Z"
    },
    "papermill": {
     "duration": 2483.801196,
     "end_time": "2021-01-24T07:15:38.330315",
     "exception": false,
     "start_time": "2021-01-24T06:34:14.529119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 2.2389 - accuracy: 0.3901\n",
      "Epoch 2/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 1.3736 - accuracy: 0.5447\n",
      "Epoch 3/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 1.1558 - accuracy: 0.6079\n",
      "Epoch 4/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 1.0322 - accuracy: 0.6355\n",
      "Epoch 5/200\n",
      "202/202 [==============================] - 12s 57ms/step - loss: 0.9771 - accuracy: 0.6598\n",
      "Epoch 6/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.9188 - accuracy: 0.6773\n",
      "Epoch 7/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.8518 - accuracy: 0.7027\n",
      "Epoch 8/200\n",
      "202/202 [==============================] - 12s 57ms/step - loss: 0.8384 - accuracy: 0.7070\n",
      "Epoch 9/200\n",
      "202/202 [==============================] - 13s 63ms/step - loss: 0.7838 - accuracy: 0.7218\n",
      "Epoch 10/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.7571 - accuracy: 0.7342\n",
      "Epoch 11/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.7549 - accuracy: 0.7277\n",
      "Epoch 12/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.7229 - accuracy: 0.7427\n",
      "Epoch 13/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.7118 - accuracy: 0.7476\n",
      "Epoch 14/200\n",
      "202/202 [==============================] - 13s 63ms/step - loss: 0.6803 - accuracy: 0.7536\n",
      "Epoch 15/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.6617 - accuracy: 0.7625\n",
      "Epoch 16/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.6444 - accuracy: 0.7695\n",
      "Epoch 17/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.6417 - accuracy: 0.7660\n",
      "Epoch 18/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.6437 - accuracy: 0.7676\n",
      "Epoch 19/200\n",
      "202/202 [==============================] - 13s 64ms/step - loss: 0.6268 - accuracy: 0.7709\n",
      "Epoch 20/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 0.6105 - accuracy: 0.7778\n",
      "Epoch 21/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.5957 - accuracy: 0.7817\n",
      "Epoch 22/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.5988 - accuracy: 0.7841\n",
      "Epoch 23/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.5671 - accuracy: 0.7887\n",
      "Epoch 24/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.5703 - accuracy: 0.7895\n",
      "Epoch 25/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.5542 - accuracy: 0.8047\n",
      "Epoch 26/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.5548 - accuracy: 0.7962\n",
      "Epoch 27/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.5520 - accuracy: 0.8036\n",
      "Epoch 28/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.5345 - accuracy: 0.8092\n",
      "Epoch 29/200\n",
      "202/202 [==============================] - 14s 68ms/step - loss: 0.5330 - accuracy: 0.8083\n",
      "Epoch 30/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.5392 - accuracy: 0.8030\n",
      "Epoch 31/200\n",
      "202/202 [==============================] - 13s 62ms/step - loss: 0.5164 - accuracy: 0.8128\n",
      "Epoch 32/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.5171 - accuracy: 0.8145\n",
      "Epoch 33/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.5137 - accuracy: 0.8057\n",
      "Epoch 34/200\n",
      "202/202 [==============================] - 13s 65ms/step - loss: 0.4938 - accuracy: 0.8156\n",
      "Epoch 35/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.4972 - accuracy: 0.8151\n",
      "Epoch 36/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.4930 - accuracy: 0.8235\n",
      "Epoch 37/200\n",
      "202/202 [==============================] - 13s 66ms/step - loss: 0.4902 - accuracy: 0.8195\n",
      "Epoch 38/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.4923 - accuracy: 0.8164\n",
      "Epoch 39/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.4677 - accuracy: 0.8229\n",
      "Epoch 40/200\n",
      "202/202 [==============================] - 13s 65ms/step - loss: 0.4515 - accuracy: 0.8296\n",
      "Epoch 41/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.4625 - accuracy: 0.8293\n",
      "Epoch 42/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.4477 - accuracy: 0.8377\n",
      "Epoch 43/200\n",
      "202/202 [==============================] - 14s 69ms/step - loss: 0.4312 - accuracy: 0.8406\n",
      "Epoch 44/200\n",
      "202/202 [==============================] - 12s 57ms/step - loss: 0.4460 - accuracy: 0.8363\n",
      "Epoch 45/200\n",
      "202/202 [==============================] - 14s 69ms/step - loss: 0.4457 - accuracy: 0.8322\n",
      "Epoch 46/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.4260 - accuracy: 0.8436\n",
      "Epoch 47/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.4228 - accuracy: 0.8440\n",
      "Epoch 48/200\n",
      "202/202 [==============================] - 15s 72ms/step - loss: 0.4262 - accuracy: 0.8454\n",
      "Epoch 49/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.4162 - accuracy: 0.8460\n",
      "Epoch 50/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.4133 - accuracy: 0.8453\n",
      "Epoch 51/200\n",
      "202/202 [==============================] - 14s 70ms/step - loss: 0.4128 - accuracy: 0.8401\n",
      "Epoch 52/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.4154 - accuracy: 0.8468\n",
      "Epoch 53/200\n",
      "202/202 [==============================] - 13s 63ms/step - loss: 0.4040 - accuracy: 0.8504\n",
      "Epoch 54/200\n",
      "202/202 [==============================] - 14s 67ms/step - loss: 0.4072 - accuracy: 0.8540\n",
      "Epoch 55/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.3970 - accuracy: 0.8557\n",
      "Epoch 56/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3869 - accuracy: 0.8589\n",
      "Epoch 57/200\n",
      "202/202 [==============================] - 14s 70ms/step - loss: 0.3832 - accuracy: 0.8597\n",
      "Epoch 58/200\n",
      "202/202 [==============================] - 13s 62ms/step - loss: 0.3892 - accuracy: 0.8568\n",
      "Epoch 59/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3789 - accuracy: 0.8619\n",
      "Epoch 60/200\n",
      "202/202 [==============================] - 13s 66ms/step - loss: 0.3835 - accuracy: 0.8585\n",
      "Epoch 61/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3776 - accuracy: 0.8610\n",
      "Epoch 62/200\n",
      "202/202 [==============================] - 13s 63ms/step - loss: 0.3695 - accuracy: 0.8627\n",
      "Epoch 63/200\n",
      "202/202 [==============================] - 13s 67ms/step - loss: 0.3703 - accuracy: 0.8633\n",
      "Epoch 64/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.3688 - accuracy: 0.8613\n",
      "Epoch 65/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.3626 - accuracy: 0.8664\n",
      "Epoch 66/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.3596 - accuracy: 0.8689\n",
      "Epoch 67/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.3581 - accuracy: 0.8728\n",
      "Epoch 68/200\n",
      "202/202 [==============================] - 15s 73ms/step - loss: 0.3441 - accuracy: 0.8757\n",
      "Epoch 69/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.3486 - accuracy: 0.8697\n",
      "Epoch 70/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.3525 - accuracy: 0.8672\n",
      "Epoch 71/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3347 - accuracy: 0.8742\n",
      "Epoch 72/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.3433 - accuracy: 0.8703\n",
      "Epoch 73/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.3509 - accuracy: 0.8740\n",
      "Epoch 74/200\n",
      "202/202 [==============================] - 14s 72ms/step - loss: 0.3482 - accuracy: 0.8737\n",
      "Epoch 75/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.3395 - accuracy: 0.8754\n",
      "Epoch 76/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.3354 - accuracy: 0.8794\n",
      "Epoch 77/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.3423 - accuracy: 0.8743\n",
      "Epoch 78/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.3342 - accuracy: 0.8767\n",
      "Epoch 79/200\n",
      "202/202 [==============================] - 15s 72ms/step - loss: 0.3175 - accuracy: 0.8805\n",
      "Epoch 80/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3310 - accuracy: 0.8765\n",
      "Epoch 81/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.3183 - accuracy: 0.8846\n",
      "Epoch 82/200\n",
      "202/202 [==============================] - 13s 64ms/step - loss: 0.3274 - accuracy: 0.8791\n",
      "Epoch 83/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.3223 - accuracy: 0.8844\n",
      "Epoch 84/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3066 - accuracy: 0.8881\n",
      "Epoch 85/200\n",
      "202/202 [==============================] - 14s 71ms/step - loss: 0.3090 - accuracy: 0.8869\n",
      "Epoch 86/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.3018 - accuracy: 0.8883\n",
      "Epoch 87/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.3163 - accuracy: 0.8899\n",
      "Epoch 88/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2984 - accuracy: 0.8927\n",
      "Epoch 89/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.3079 - accuracy: 0.8849\n",
      "Epoch 90/200\n",
      "202/202 [==============================] - 13s 65ms/step - loss: 0.3088 - accuracy: 0.8891\n",
      "Epoch 91/200\n",
      "202/202 [==============================] - 14s 72ms/step - loss: 0.2931 - accuracy: 0.8913\n",
      "Epoch 92/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.2918 - accuracy: 0.8911\n",
      "Epoch 93/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2976 - accuracy: 0.8889\n",
      "Epoch 94/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2988 - accuracy: 0.8909\n",
      "Epoch 95/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.2814 - accuracy: 0.8968\n",
      "Epoch 96/200\n",
      "202/202 [==============================] - 16s 78ms/step - loss: 0.2976 - accuracy: 0.8872\n",
      "Epoch 97/200\n",
      "202/202 [==============================] - 12s 62ms/step - loss: 0.2806 - accuracy: 0.8965\n",
      "Epoch 98/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2815 - accuracy: 0.8981\n",
      "Epoch 99/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2751 - accuracy: 0.9007\n",
      "Epoch 100/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2752 - accuracy: 0.9046\n",
      "Epoch 101/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2760 - accuracy: 0.8944\n",
      "Epoch 102/200\n",
      "202/202 [==============================] - 15s 76ms/step - loss: 0.2723 - accuracy: 0.9004\n",
      "Epoch 103/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.2648 - accuracy: 0.9009\n",
      "Epoch 104/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2719 - accuracy: 0.8987\n",
      "Epoch 105/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2507 - accuracy: 0.9038\n",
      "Epoch 106/200\n",
      "202/202 [==============================] - 12s 62ms/step - loss: 0.2760 - accuracy: 0.9034\n",
      "Epoch 107/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.2603 - accuracy: 0.9048\n",
      "Epoch 108/200\n",
      "202/202 [==============================] - 15s 76ms/step - loss: 0.2553 - accuracy: 0.9071\n",
      "Epoch 109/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 0.2451 - accuracy: 0.9099\n",
      "Epoch 110/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2621 - accuracy: 0.9048\n",
      "Epoch 111/200\n",
      "202/202 [==============================] - 13s 62ms/step - loss: 0.2480 - accuracy: 0.9108\n",
      "Epoch 112/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2419 - accuracy: 0.9096\n",
      "Epoch 113/200\n",
      "202/202 [==============================] - 14s 71ms/step - loss: 0.2429 - accuracy: 0.9119\n",
      "Epoch 114/200\n",
      "202/202 [==============================] - 14s 67ms/step - loss: 0.2392 - accuracy: 0.9130\n",
      "Epoch 115/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.2540 - accuracy: 0.9068\n",
      "Epoch 116/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.2474 - accuracy: 0.9099\n",
      "Epoch 117/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2354 - accuracy: 0.9132\n",
      "Epoch 118/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 0.2315 - accuracy: 0.9149\n",
      "Epoch 119/200\n",
      "202/202 [==============================] - 16s 78ms/step - loss: 0.2543 - accuracy: 0.9073\n",
      "Epoch 120/200\n",
      "202/202 [==============================] - 13s 64ms/step - loss: 0.2327 - accuracy: 0.9155\n",
      "Epoch 121/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.2503 - accuracy: 0.9083\n",
      "Epoch 122/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2317 - accuracy: 0.9178\n",
      "Epoch 123/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2378 - accuracy: 0.9169\n",
      "Epoch 124/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2421 - accuracy: 0.9110\n",
      "Epoch 125/200\n",
      "202/202 [==============================] - 17s 82ms/step - loss: 0.2210 - accuracy: 0.9184\n",
      "Epoch 126/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.2161 - accuracy: 0.9256\n",
      "Epoch 127/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2343 - accuracy: 0.9139\n",
      "Epoch 128/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2281 - accuracy: 0.9197\n",
      "Epoch 129/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.2259 - accuracy: 0.9197\n",
      "Epoch 130/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2180 - accuracy: 0.9215\n",
      "Epoch 131/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2263 - accuracy: 0.9172\n",
      "Epoch 132/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2065 - accuracy: 0.9281\n",
      "Epoch 133/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2084 - accuracy: 0.9253\n",
      "Epoch 134/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 0.2138 - accuracy: 0.9245\n",
      "Epoch 135/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.2228 - accuracy: 0.9226\n",
      "Epoch 136/200\n",
      "202/202 [==============================] - 16s 80ms/step - loss: 0.2179 - accuracy: 0.9212\n",
      "Epoch 137/200\n",
      "202/202 [==============================] - 12s 62ms/step - loss: 0.2067 - accuracy: 0.9256\n",
      "Epoch 138/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.2073 - accuracy: 0.9265\n",
      "Epoch 139/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.2024 - accuracy: 0.9237\n",
      "Epoch 140/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2138 - accuracy: 0.9242\n",
      "Epoch 141/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2055 - accuracy: 0.9281\n",
      "Epoch 142/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2052 - accuracy: 0.9250\n",
      "Epoch 143/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.2026 - accuracy: 0.9275\n",
      "Epoch 144/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1951 - accuracy: 0.9271\n",
      "Epoch 145/200\n",
      "202/202 [==============================] - 13s 63ms/step - loss: 0.1973 - accuracy: 0.9267\n",
      "Epoch 146/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1887 - accuracy: 0.9316\n",
      "Epoch 147/200\n",
      "202/202 [==============================] - 16s 82ms/step - loss: 0.1882 - accuracy: 0.9326\n",
      "Epoch 148/200\n",
      "202/202 [==============================] - 13s 62ms/step - loss: 0.1916 - accuracy: 0.9313\n",
      "Epoch 149/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.2025 - accuracy: 0.9296\n",
      "Epoch 150/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1948 - accuracy: 0.9293\n",
      "Epoch 151/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.1894 - accuracy: 0.9298\n",
      "Epoch 152/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.1748 - accuracy: 0.9366\n",
      "Epoch 153/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.1913 - accuracy: 0.9307\n",
      "Epoch 154/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1877 - accuracy: 0.9321\n",
      "Epoch 155/200\n",
      "202/202 [==============================] - 12s 57ms/step - loss: 0.1820 - accuracy: 0.9324\n",
      "Epoch 156/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1813 - accuracy: 0.9346\n",
      "Epoch 157/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.1862 - accuracy: 0.9304\n",
      "Epoch 158/200\n",
      "202/202 [==============================] - 16s 77ms/step - loss: 0.1795 - accuracy: 0.9355\n",
      "Epoch 159/200\n",
      "202/202 [==============================] - 14s 72ms/step - loss: 0.1744 - accuracy: 0.9396\n",
      "Epoch 160/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1882 - accuracy: 0.9326\n",
      "Epoch 161/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.1731 - accuracy: 0.9349\n",
      "Epoch 162/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1880 - accuracy: 0.9320\n",
      "Epoch 163/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1803 - accuracy: 0.9360\n",
      "Epoch 164/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1823 - accuracy: 0.9340\n",
      "Epoch 165/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1645 - accuracy: 0.9424\n",
      "Epoch 166/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1690 - accuracy: 0.9416\n",
      "Epoch 167/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1850 - accuracy: 0.9285\n",
      "Epoch 168/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1704 - accuracy: 0.9408\n",
      "Epoch 169/200\n",
      "202/202 [==============================] - 16s 79ms/step - loss: 0.1694 - accuracy: 0.9419\n",
      "Epoch 170/200\n",
      "202/202 [==============================] - 15s 74ms/step - loss: 0.1726 - accuracy: 0.9372\n",
      "Epoch 171/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.1664 - accuracy: 0.9421\n",
      "Epoch 172/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1666 - accuracy: 0.9421\n",
      "Epoch 173/200\n",
      "202/202 [==============================] - 12s 62ms/step - loss: 0.1567 - accuracy: 0.9433\n",
      "Epoch 174/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1636 - accuracy: 0.9419\n",
      "Epoch 175/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1607 - accuracy: 0.9377\n",
      "Epoch 176/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.1631 - accuracy: 0.9425\n",
      "Epoch 177/200\n",
      "202/202 [==============================] - 12s 57ms/step - loss: 0.1530 - accuracy: 0.9459\n",
      "Epoch 178/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1742 - accuracy: 0.9355\n",
      "Epoch 179/200\n",
      "202/202 [==============================] - 13s 62ms/step - loss: 0.1622 - accuracy: 0.9424\n",
      "Epoch 180/200\n",
      "202/202 [==============================] - 14s 68ms/step - loss: 0.1599 - accuracy: 0.9424\n",
      "Epoch 181/200\n",
      "202/202 [==============================] - 17s 85ms/step - loss: 0.1555 - accuracy: 0.9438\n",
      "Epoch 182/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 0.1633 - accuracy: 0.9414\n",
      "Epoch 183/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.1559 - accuracy: 0.9422\n",
      "Epoch 184/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1525 - accuracy: 0.9476\n",
      "Epoch 185/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.1595 - accuracy: 0.9452\n",
      "Epoch 186/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1611 - accuracy: 0.9425\n",
      "Epoch 187/200\n",
      "202/202 [==============================] - 11s 55ms/step - loss: 0.1556 - accuracy: 0.9480\n",
      "Epoch 188/200\n",
      "202/202 [==============================] - 12s 60ms/step - loss: 0.1550 - accuracy: 0.9441\n",
      "Epoch 189/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1586 - accuracy: 0.9456\n",
      "Epoch 190/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1489 - accuracy: 0.9473\n",
      "Epoch 191/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1496 - accuracy: 0.9467\n",
      "Epoch 192/200\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1450 - accuracy: 0.9469\n",
      "Epoch 193/200\n",
      "202/202 [==============================] - 13s 63ms/step - loss: 0.1554 - accuracy: 0.9467\n",
      "Epoch 194/200\n",
      "202/202 [==============================] - 12s 58ms/step - loss: 0.1553 - accuracy: 0.9442\n",
      "Epoch 195/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1396 - accuracy: 0.9509\n",
      "Epoch 196/200\n",
      "202/202 [==============================] - 11s 57ms/step - loss: 0.1433 - accuracy: 0.9481\n",
      "Epoch 197/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.1462 - accuracy: 0.9481\n",
      "Epoch 198/200\n",
      "202/202 [==============================] - 12s 61ms/step - loss: 0.1474 - accuracy: 0.9503\n",
      "Epoch 199/200\n",
      "202/202 [==============================] - 12s 59ms/step - loss: 0.1366 - accuracy: 0.9484\n",
      "Epoch 200/200\n",
      "202/202 [==============================] - 11s 56ms/step - loss: 0.1324 - accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit_generator(train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                               steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T07:16:01.739629Z",
     "iopub.status.busy": "2021-01-24T07:16:01.738649Z",
     "iopub.status.idle": "2021-01-24T07:16:21.522538Z",
     "shell.execute_reply": "2021-01-24T07:16:21.521917Z"
    },
    "papermill": {
     "duration": 31.948279,
     "end_time": "2021-01-24T07:16:21.522669",
     "exception": false,
     "start_time": "2021-01-24T07:15:49.574390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "test_images = os.listdir('../input/hackerearth-deep-learning-challenge-holidayseason/dataset/test/')\n",
    "test_img = []\n",
    "for image in test_images:\n",
    "    test_img.append(read_img('../input/hackerearth-deep-learning-challenge-holidayseason/dataset/test/' + image))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T07:16:44.702461Z",
     "iopub.status.busy": "2021-01-24T07:16:44.701052Z",
     "iopub.status.idle": "2021-01-24T07:16:47.012943Z",
     "shell.execute_reply": "2021-01-24T07:16:47.011659Z"
    },
    "papermill": {
     "duration": 13.629776,
     "end_time": "2021-01-24T07:16:47.013058",
     "exception": false,
     "start_time": "2021-01-24T07:16:33.383282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store image in form of matrices and predict the test_img\n",
    "x_test = np.array(test_img, np.float32) / 255\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T07:17:11.133341Z",
     "iopub.status.busy": "2021-01-24T07:17:11.130203Z",
     "iopub.status.idle": "2021-01-24T07:17:11.135544Z",
     "shell.execute_reply": "2021-01-24T07:17:11.135932Z"
    },
    "papermill": {
     "duration": 12.878395,
     "end_time": "2021-01-24T07:17:11.136054",
     "exception": false,
     "start_time": "2021-01-24T07:16:58.257659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=1)\n",
    "rev_y = {v:k for k,v in label_numeric.items()}\n",
    "pred_labels = [rev_y[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T07:17:33.500751Z",
     "iopub.status.busy": "2021-01-24T07:17:33.500046Z",
     "iopub.status.idle": "2021-01-24T07:17:33.747108Z",
     "shell.execute_reply": "2021-01-24T07:17:33.746478Z"
    },
    "papermill": {
     "duration": 11.413777,
     "end_time": "2021-01-24T07:17:33.747209",
     "exception": false,
     "start_time": "2021-01-24T07:17:22.333432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image7761.jpg</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image3202.jpg</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image688.jpg</td>\n",
       "      <td>Snowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image233.jpg</td>\n",
       "      <td>Candle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image4332.jpg</td>\n",
       "      <td>Christmas_Tree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image           Class\n",
       "0  image7761.jpg   Miscellaneous\n",
       "1  image3202.jpg   Miscellaneous\n",
       "2   image688.jpg         Snowman\n",
       "3   image233.jpg          Candle\n",
       "4  image4332.jpg  Christmas_Tree"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the prediction\n",
    "sub = pd.DataFrame({'Image': test_images, 'Class': pred_labels})\n",
    "sub.to_csv('submission.csv', index = False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 11.242458,
     "end_time": "2021-01-24T07:17:56.753444",
     "exception": false,
     "start_time": "2021-01-24T07:17:45.510986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2695.309514,
   "end_time": "2021-01-24T07:18:10.994181",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-24T06:33:15.684667",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
